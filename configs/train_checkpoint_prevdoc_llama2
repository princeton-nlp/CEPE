--model_name_or_path output/Llama-2-7b-hf-dup_concate-initzero-lr5e-4-n_cn_layer32-chunk_size256-n_ctx4-ctx_size64-total_bs128-bs_1-grad_acc32-train_foldertrain-eval_foldereval_1k-train_load_stratduplicate-eval_window256/checkpoint-4000
--num_cross_attn_layers 32
--train_file data/10m_sample_4k_chunks_ab/train
--train_domains arxiv,book
--do_train True
--validation_file data/1m_sample_4k_chunks_ab/eval
--validation_domains arxiv,book
--eval_window 256
--do_eval True
--chunk_size 2048
--num_context 8
--context_size 256
--bf16 True
--torch_dtype bfloat16
--max_steps 8000
--init_mode none
--per_device_train_batch_size 2
--per_device_eval_batch_size 4
--gradient_accumulation_steps 16
--optim "adamw_torch"
--learning_rate 3e-4
--weight_decay 0
--warmup_ratio 0.04
--lr_scheduler_type "cosine"
--tf32 True
--evaluation_strategy "steps"
--eval_steps 400
--save_strategy "steps"
--save_steps 400
--save_total_limit 2
--logging_steps 10
--dataloader_num_workers 4
--ddp_find_unused_parameters False
 
