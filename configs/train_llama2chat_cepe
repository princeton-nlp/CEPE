--model_name_or_path output/Llama-2-7b-dup
--num_cross_attn_layers 32
--train_file data/ab_8k/train,data/rp_concat_8k/train
--train_file_distill data/8k_ab/train-logits-enc2048-dec2048-Llama-2-7b-chat-hf,data/8k_concat/train-logits-enc2048-dec2048-Llama-2-7b-chat-hf
--train_domains arxiv,book;arxiv,book,c4-rp,cc,github,stackexchange,wiki
--do_train True
--validation_file data/ab_8k/test,data/rp_concat_8k/test
--validation_file_distill data/8k_ab/eval-logits-enc2048-dec2048-Llama-2-7b-chat-hf,data/8k_concat/eval-logits-enc2048-dec2048-Llama-2-7b-chat-hf
--train_domains arxiv,book;arxiv,book,c4-rp,cc,github,stackexchange,wiki
--eval_window 256
--do_eval True
--chunk_size 2048
--num_context 8
--context_size 256
--bf16 True
--torch_dtype bfloat16
--max_steps 20000
--init_mode none
--per_device_train_batch_size 1
--per_device_eval_batch_size 4
--gradient_accumulation_steps 16
--optim "adamw_torch"
--learning_rate 3e-4
--weight_decay 0
--warmup_ratio 0.04
--lr_scheduler_type "cosine"
--tf32 True
--evaluation_strategy "steps"
--eval_steps 1000
--save_strategy "steps"
--save_steps 1000
--save_total_limit 2
--logging_steps 10
--dataloader_num_workers 4
--ddp_find_unused_parameters False
 