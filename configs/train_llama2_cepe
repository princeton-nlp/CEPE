--model_name_or_path output/Llama-2-7b-chat-dup
--num_cross_attn_layers 32
--train_file data/ab_8k/train,data/rp_concat_8k/train
--train_domains arxiv,book;arxiv,book,c4-rp,cc,github,stackexchange,wiki
--do_train True
--train_encoder True
--validation_file data/ab_8k/test,data/rp_concat_8k/test
--train_domains arxiv,book;arxiv,book,c4-rp,cc,github,stackexchange,wiki
--eval_window 256
--do_eval True
--chunk_size 4096
--num_context 16
--context_size 256
--bf16 True
--torch_dtype bfloat16
--max_steps 20000
--init_mode none
--per_device_train_batch_size 1
--per_device_eval_batch_size 4
--gradient_accumulation_steps 16
--optim "adamw_torch"
--learning_rate 3e-4
--weight_decay 0
--warmup_ratio 0.04
--lr_scheduler_type "cosine"
--tf32 True
--evaluation_strategy "steps"
--eval_steps 1000
--save_strategy "steps"
--save_steps 1000
--save_total_limit 2
--logging_steps 10
--dataloader_num_workers 4
--ddp_find_unused_parameters False
--kl_loss_cof 2
--lm_loss_cof 1
--kl_loss_mode drop
 