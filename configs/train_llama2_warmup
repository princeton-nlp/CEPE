--output_dir output/Llama-2-7b-dup
--model_name_or_path meta-llama/Llama-2-7b-hf
--train_file ../p-cross-att/data/2point5m_sample_8k_chunks_concatenate/train 
--train_load_strategy duplicate
--train_domains arxiv,book,c4-rp,cc,github,stackexchange,wiki
--validation_file ../p-cross-att/data/2point5m_sample_8k_chunks_concatenate/eval_1k 
--validation_load_strategy duplicate
--validation_domains arxiv,book,c4-rp,cc,github,stackexchange,wiki
--eval_window 256
--num_cross_attn_layers 32 
--num_context 4 
--context_size 64 
--learning_rate 5e-4 
--chunk_size 256 
--per_device_train_batch_size 2
--torch_dtype bfloat16 
--save_steps 500 
--logging_steps 10 
--evaluation_strategy steps
--eval_steps 500 
--do_train True 
--do_eval True 
--overwrite_output_dir 
--encoder_name_or_path hyen/LLaMA-MLM-Large
--encoder_name_or_path /scratch/gpfs/hyen/p-cross-att/output/llama-large-mlm/checkpoint-100000
--train_encoder True 
--gradient_accumulation_steps 32
--ddp_find_unused_parameters False
--max_steps 4000
--bf16 True
--init_mode copy
--optim adamw_torch
--weight_decay 0
--warmup_ratio 0.04
--lr_scheduler_type cosine
--prediction_loss_only True
--save_total_limit 2
--dataloader_num_workers 4