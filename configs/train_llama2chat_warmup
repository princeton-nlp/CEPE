--output_dir output/Llama-2-7b-chat-dup
--model_name_or_path meta-llama/Llama-2-7b-chat-hf
--train_file data/rp_concat_8k/train
--train_load_strategy duplicate
--train_domains arxiv,book,c4-rp,cc,github,stackexchange,wiki
--validation_file data/rp_concat_8k/test
--validation_load_strategy duplicate
--validation_domains arxiv,book,c4-rp,cc,github,stackexchange,wiki
--eval_window 256
--num_cross_attn_layers 32 
--num_context 4 
--context_size 64 
--learning_rate 5e-4 
--chunk_size 256 
--per_device_train_batch_size 2
--torch_dtype bfloat16 
--save_steps 500 
--logging_steps 10 
--evaluation_strategy steps
--eval_steps 500 
--do_train True 
--do_eval True 
--overwrite_output_dir 
--encoder_name_or_path hyen/LLaMA-MLM-Large
--train_encoder True 
--gradient_accumulation_steps 32
--ddp_find_unused_parameters False
--max_steps 4000
--bf16 True
--init_mode copy
--optim adamw_torch
--weight_decay 0
--warmup_ratio 0.04
--lr_scheduler_type cosine
--prediction_loss_only True
--save_total_limit 2
--dataloader_num_workers 4