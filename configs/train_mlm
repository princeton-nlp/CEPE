--config_name configs/mlm_config.json
--tokenizer_name meta-llama/Llama-2-7b-hf
--train_file data/rp_concat_8k/train
--per_device_train_batch_size 16
--gradient_accumulation_steps 16
--learning_rate 1e-3
--do_train True
--validation_file data/rp_concat_8k/eval
--do_eval True
--save_steps 5000
--save_total_limit 2
--bf16 True
--torch_dtype bfloat16
--optim "adamw_torch"
--lr_scheduler_type "cosine"
--warmup_ratio 0.04
--max_seq_length 512
--mlm_probability 0.3
--max_steps 100000
--dataloader_num_workers 4
--output_dir output/LLaMA-MLM-Large 
