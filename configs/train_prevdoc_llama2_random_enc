--model_name_or_path /scratch/gpfs/hyen/models/Llama-2-7b-hf
--num_cross_attn_layers 32
--train_file data/5m_sample_8k_chunks_ab/train,data/2point5m_sample_8k_chunks_concatenate/train
--train_domains arxiv,book;arxiv,book,c4-rp,cc,github,stackexchange,wiki
--do_train True
--validation_file data/5m_sample_8k_chunks_ab/eval,data/2point5m_sample_8k_chunks_concatenate/eval_1k
--validation_domains arxiv,book;arxiv,book,c4-rp,cc,github,stackexchange,wiki
--eval_window 256
--do_eval True
--chunk_size 4096
--num_context 16
--context_size 256
--bf16 True
--torch_dtype bfloat16
--max_steps 20000
--init_mode zero
--per_device_train_batch_size 1
--per_device_eval_batch_size 4
--gradient_accumulation_steps 16
--optim adamw_torch
--learning_rate 3e-4
--weight_decay 0
--warmup_ratio 0.04
--lr_scheduler_type cosine
--tf32 True
--evaluation_strategy steps
--eval_steps 2000
--save_strategy steps
--save_steps 2000
--save_total_limit 2
--logging_steps 10
--dataloader_num_workers 4
--ddp_find_unused_parameters False
--train_encoder True
--encoder_config mlm_config.json
--ddp_find_unused_parameters False
--mask_prob 0.3
--mask_seq_prob 0.1
